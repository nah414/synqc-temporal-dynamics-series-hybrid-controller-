{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6ae8ec",
   "metadata": {},
   "source": [
    "# SynQc Run Bundle Analysis\n",
    "\n",
    "This notebook loads an **exported SynQc run bundle** (JSON or CSV) and produces three analyses:\n",
    "\n",
    "1. **KPI trends over time**\n",
    "2. **Target comparisons**\n",
    "3. **“Fidelity vs latency” tradeoff**\n",
    "\n",
    "It’s intentionally **schema-tolerant**: it will flatten nested JSON (e.g. `kpis.fidelity`, `metrics.latency_ms`) and try to infer the timestamp and target columns.\n",
    "\n",
    "## What is a “run bundle” here?\n",
    "\n",
    "A “bundle” can be any of the following:\n",
    "\n",
    "- A JSON file that is a **list of runs**, e.g. `[{...}, {...}]`\n",
    "- A JSON file that is an **object with a list** under a common key like:\n",
    "  - `runs`, `experiments`, `items`, or `data`\n",
    "- A **directory** containing multiple `*.json` / `*.csv` files (they’ll be concatenated)\n",
    "\n",
    "Each *run* ideally includes:\n",
    "\n",
    "- a timestamp (e.g. `created_at`, `timestamp`, `completed_at`)\n",
    "- a target identifier (e.g. `hardware_target`, `target`, `backend`)\n",
    "- KPI values (e.g. `fidelity`, `latency_ms`)\n",
    "\n",
    "> If your exports don’t match these names, no problem—this notebook will show you the detected columns and you can override the auto-detection in the config cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e16b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# Optional: nicer inline resolution\n",
    "plt.rcParams[\"figure.dpi\"] = 130\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a01b35",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set `BUNDLE_PATH` to either:\n",
    "\n",
    "- a **JSON/CSV file**, or\n",
    "- a **directory** containing multiple JSON/CSV files.\n",
    "\n",
    "If this notebook lives in `notebooks/`, a good convention is to store exports in `exports/` at repo root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f68d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this as needed:\n",
    "# - If the notebook is in repo root, 'exports/' is typically correct.\n",
    "# - If the notebook is in 'notebooks/', '../exports/' is typically correct.\n",
    "_default_exports_dir = Path(\"exports\") if Path(\"exports\").exists() else Path(\"../exports\")\n",
    "\n",
    "BUNDLE_PATH = _default_exports_dir  # file or directory\n",
    "\n",
    "# Optional overrides (leave as None to auto-detect):\n",
    "TIME_COL_OVERRIDE: Optional[str] = None\n",
    "TARGET_COL_OVERRIDE: Optional[str] = None\n",
    "FIDELITY_COL_OVERRIDE: Optional[str] = None\n",
    "LATENCY_COL_OVERRIDE: Optional[str] = None\n",
    "\n",
    "# Time aggregation for trend plots (set to None to plot raw points):\n",
    "TREND_RESAMPLE_RULE = \"D\"  # e.g. \"H\", \"D\", \"W\", \"M\" (hour/day/week/month)\n",
    "TREND_AGG = \"mean\"         # \"mean\", \"median\", etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570fb6e",
   "metadata": {},
   "source": [
    "## Load & normalize bundle\n",
    "\n",
    "The functions below:\n",
    "\n",
    "- read JSON/CSV (or a directory of files),\n",
    "- extract a list of runs,\n",
    "- flatten nested structures into a tabular dataframe,\n",
    "- infer likely time/target/KPI columns,\n",
    "- create *canonical* columns when possible: `kpi_fidelity`, `kpi_latency_ms`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_json(path: Path) -> Any:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def _read_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def _iter_input_files(path: Path) -> List[Path]:\n",
    "    if path.is_file():\n",
    "        return [path]\n",
    "    if path.is_dir():\n",
    "        files = sorted([p for p in path.rglob(\"*\") if p.suffix.lower() in {\".json\", \".csv\"}])\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No .json/.csv files found under directory: {path.resolve()}\")\n",
    "        return files\n",
    "    raise FileNotFoundError(f\"Bundle path not found: {path.resolve()}\")\n",
    "\n",
    "def _extract_runs(obj: Any) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"Return (runs, bundle_meta).\"\"\"\n",
    "    meta: Dict[str, Any] = {}\n",
    "    if isinstance(obj, list):\n",
    "        # list of runs\n",
    "        runs = obj\n",
    "    elif isinstance(obj, dict):\n",
    "        # object wrapping list of runs\n",
    "        for key in (\"runs\", \"experiments\", \"items\", \"data\"):\n",
    "            if key in obj and isinstance(obj[key], list):\n",
    "                runs = obj[key]\n",
    "                meta = {k: v for k, v in obj.items() if k != key}\n",
    "                break\n",
    "        else:\n",
    "            # maybe a single run\n",
    "            runs = [obj]\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported JSON top-level type: {type(obj)}\")\n",
    "    # Filter to dicts\n",
    "    runs = [r for r in runs if isinstance(r, dict)]\n",
    "    return runs, meta\n",
    "\n",
    "def load_run_bundle(path: Path) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Load a run bundle from JSON/CSV file(s) and return (df, meta).\"\"\"\n",
    "    files = _iter_input_files(path)\n",
    "    all_frames: List[pd.DataFrame] = []\n",
    "    meta: Dict[str, Any] = {\"source_files\": [str(p) for p in files]}\n",
    "\n",
    "    for p in files:\n",
    "        if p.suffix.lower() == \".json\":\n",
    "            obj = _read_json(p)\n",
    "            runs, bundle_meta = _extract_runs(obj)\n",
    "            meta.setdefault(\"bundle_meta\", []).append({\"file\": str(p), \"meta\": bundle_meta})\n",
    "            df_part = pd.json_normalize(runs, sep=\".\")\n",
    "            df_part[\"_source_file\"] = str(p)\n",
    "            all_frames.append(df_part)\n",
    "        elif p.suffix.lower() == \".csv\":\n",
    "            df_part = _read_csv(p)\n",
    "            df_part[\"_source_file\"] = str(p)\n",
    "            all_frames.append(df_part)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if not all_frames:\n",
    "        raise ValueError(\"No usable data found.\")\n",
    "\n",
    "    df = pd.concat(all_frames, ignore_index=True, sort=False)\n",
    "    return df, meta\n",
    "\n",
    "def pick_first_existing(columns: Iterable[str], candidates: Iterable[str]) -> Optional[str]:\n",
    "    cols = set(columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def infer_time_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    candidates = [\n",
    "        \"timestamp\", \"time\", \"datetime\", \"date\",\n",
    "        \"created_at\", \"started_at\", \"completed_at\", \"finished_at\",\n",
    "        \"createdAt\", \"startedAt\", \"completedAt\",\n",
    "        \"server_time\", \"serverTime\",\n",
    "    ]\n",
    "    # Also match common nested paths\n",
    "    nested_candidates = [\n",
    "        \"meta.timestamp\", \"meta.created_at\", \"meta.completed_at\",\n",
    "        \"run.timestamp\", \"run.created_at\", \"run.completed_at\",\n",
    "    ]\n",
    "    col = pick_first_existing(df.columns, candidates + nested_candidates)\n",
    "    if col:\n",
    "        return col\n",
    "\n",
    "    # Fallback heuristic: any column containing 'time' or ending in '_at'\n",
    "    heur = [c for c in df.columns if re.search(r\"(time|_at)$\", c, re.IGNORECASE)]\n",
    "    return heur[0] if heur else None\n",
    "\n",
    "def infer_target_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    candidates = [\n",
    "        \"hardware_target\", \"target\", \"backend\", \"provider\", \"device\",\n",
    "        \"hardware.target\", \"hardware.backend\", \"hardware.name\",\n",
    "        \"target.name\", \"target_id\", \"backend_name\",\n",
    "        \"provider_target\", \"qpu\", \"qpu_name\",\n",
    "    ]\n",
    "    col = pick_first_existing(df.columns, candidates)\n",
    "    if col:\n",
    "        return col\n",
    "\n",
    "    # Heuristic: contains 'target' or 'backend'\n",
    "    heur = [c for c in df.columns if re.search(r\"(target|backend|qpu)\", c, re.IGNORECASE)]\n",
    "    return heur[0] if heur else None\n",
    "\n",
    "def infer_kpi_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:\n",
    "    fidelity_candidates = [\n",
    "        \"kpi_fidelity\", \"fidelity\",\n",
    "        \"kpis.fidelity\", \"kpi.fidelity\", \"metrics.fidelity\", \"results.fidelity\",\n",
    "        \"kpis.fidelity_score\", \"kpis.fidelityScore\",\n",
    "    ]\n",
    "    latency_candidates = [\n",
    "        \"kpi_latency_ms\", \"latency_ms\", \"latencyMs\",\n",
    "        \"kpis.latency_ms\", \"kpis.latencyMs\", \"kpi.latency_ms\",\n",
    "        \"metrics.latency_ms\", \"results.latency_ms\",\n",
    "        \"duration_ms\", \"runtime_ms\", \"execution_time_ms\",\n",
    "        \"latency\", \"duration\", \"runtime\",\n",
    "    ]\n",
    "    return {\n",
    "        \"fidelity\": pick_first_existing(df.columns, fidelity_candidates),\n",
    "        \"latency\": pick_first_existing(df.columns, latency_candidates),\n",
    "    }\n",
    "\n",
    "def coerce_datetime(series: pd.Series) -> pd.Series:\n",
    "    # pandas can parse many formats; errors become NaT\n",
    "    dt = pd.to_datetime(series, errors=\"coerce\", utc=True)\n",
    "    # drop timezone for easier plotting\n",
    "    try:\n",
    "        dt = dt.dt.tz_convert(None)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return dt\n",
    "\n",
    "def coerce_numeric(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def prepare_dataframe(df_raw: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    df = df_raw.copy()\n",
    "    info: Dict[str, Any] = {}\n",
    "\n",
    "    # Decide time/target/KPI columns (allow override)\n",
    "    time_col = TIME_COL_OVERRIDE or infer_time_column(df)\n",
    "    target_col = TARGET_COL_OVERRIDE or infer_target_column(df)\n",
    "    kpis = infer_kpi_columns(df)\n",
    "    fidelity_col = FIDELITY_COL_OVERRIDE or kpis[\"fidelity\"]\n",
    "    latency_col = LATENCY_COL_OVERRIDE or kpis[\"latency\"]\n",
    "\n",
    "    info[\"time_col\"] = time_col\n",
    "    info[\"target_col\"] = target_col\n",
    "    info[\"fidelity_col\"] = fidelity_col\n",
    "    info[\"latency_col\"] = latency_col\n",
    "\n",
    "    # Coerce time\n",
    "    if time_col and time_col in df.columns:\n",
    "        df[time_col] = coerce_datetime(df[time_col])\n",
    "\n",
    "    # Coerce likely numeric KPI columns\n",
    "    for c in [fidelity_col, latency_col]:\n",
    "        if c and c in df.columns:\n",
    "            df[c] = coerce_numeric(df[c])\n",
    "\n",
    "    # Create canonical KPI columns if we found something\n",
    "    if fidelity_col and fidelity_col in df.columns:\n",
    "        df[\"kpi_fidelity\"] = df[fidelity_col]\n",
    "    if latency_col and latency_col in df.columns:\n",
    "        # If the chosen latency column looks like seconds, convert -> ms\n",
    "        if re.search(r\"(\\bsec\\b|_s$|seconds)\", latency_col, re.IGNORECASE):\n",
    "            df[\"kpi_latency_ms\"] = df[latency_col] * 1000.0\n",
    "            info[\"latency_units\"] = \"s (converted to ms)\"\n",
    "        else:\n",
    "            df[\"kpi_latency_ms\"] = df[latency_col]\n",
    "            info[\"latency_units\"] = \"ms (assumed)\"\n",
    "\n",
    "    # Canonical target column name (optional)\n",
    "    if target_col and target_col in df.columns:\n",
    "        df[\"hardware_target_canonical\"] = df[target_col].astype(str)\n",
    "    else:\n",
    "        df[\"hardware_target_canonical\"] = \"unknown\"\n",
    "\n",
    "    # Canonical time column name (optional)\n",
    "    if time_col and time_col in df.columns:\n",
    "        df[\"timestamp_canonical\"] = df[time_col]\n",
    "    else:\n",
    "        df[\"timestamp_canonical\"] = pd.NaT\n",
    "\n",
    "    # Sort by time if possible\n",
    "    if df[\"timestamp_canonical\"].notna().any():\n",
    "        df = df.sort_values(\"timestamp_canonical\")\n",
    "\n",
    "    return df, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw, bundle_meta = load_run_bundle(BUNDLE_PATH)\n",
    "df, detected = prepare_dataframe(df_raw)\n",
    "\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(\"Detected columns:\", json.dumps(detected, indent=2))\n",
    "print(\"Unique targets:\", df[\"hardware_target_canonical\"].nunique())\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a582e5b",
   "metadata": {},
   "source": [
    "## Inspect KPI columns\n",
    "\n",
    "Below we list numeric columns to help you decide which KPIs you want to trend/compare.\n",
    "\n",
    "If the auto-detection didn’t find your fidelity/latency columns, you can set `FIDELITY_COL_OVERRIDE` and/or `LATENCY_COL_OVERRIDE` in the config cell and re-run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ded1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [c for c in df.select_dtypes(include=[\"number\"]).columns]\n",
    "numeric_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary (time range, counts per target)\n",
    "if df[\"timestamp_canonical\"].notna().any():\n",
    "    print(\"Time range:\",\n",
    "          df[\"timestamp_canonical\"].min(),\n",
    "          \"→\",\n",
    "          df[\"timestamp_canonical\"].max())\n",
    "else:\n",
    "    print(\"No parsable timestamps found (timestamp_canonical is all NaT).\")\n",
    "\n",
    "display(df[\"hardware_target_canonical\"].value_counts().head(20))\n",
    "df.describe(include=\"all\").T.head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919ff8f",
   "metadata": {},
   "source": [
    "## Plot helpers\n",
    "\n",
    "We keep the plotting functions small and matplotlib-only (no seaborn), so they work in minimal environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kpi_trend_over_time(\n",
    "    df: pd.DataFrame,\n",
    "    kpi_col: str,\n",
    "    time_col: str = \"timestamp_canonical\",\n",
    "    target_col: str = \"hardware_target_canonical\",\n",
    "    resample_rule: Optional[str] = \"D\",\n",
    "    agg: str = \"mean\",\n",
    "    title: Optional[str] = None,\n",
    "):\n",
    "    data = df[[time_col, target_col, kpi_col]].dropna()\n",
    "    if data.empty:\n",
    "        print(f\"No data to plot for {kpi_col}.\")\n",
    "        return\n",
    "\n",
    "    # Ensure time is datetime\n",
    "    data = data.copy()\n",
    "    data[time_col] = pd.to_datetime(data[time_col], errors=\"coerce\")\n",
    "    data = data.dropna(subset=[time_col])\n",
    "    if data.empty:\n",
    "        print(f\"No valid timestamps to plot for {kpi_col}.\")\n",
    "        return\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    if resample_rule:\n",
    "        data = data.set_index(time_col)\n",
    "        for tgt, g in data.groupby(target_col):\n",
    "            series = getattr(g[kpi_col].resample(resample_rule), agg)()\n",
    "            if series.dropna().empty:\n",
    "                continue\n",
    "            plt.plot(series.index, series.values, label=str(tgt))\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(kpi_col)\n",
    "        plt.title(title or f\"{kpi_col} trend over time ({agg}, resample={resample_rule})\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.xticks(rotation=30, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Plot raw points\n",
    "        for tgt, g in data.groupby(target_col):\n",
    "            plt.plot(g[time_col], g[kpi_col], marker=\"o\", linestyle=\"-\", label=str(tgt))\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(kpi_col)\n",
    "        plt.title(title or f\"{kpi_col} trend over time (raw)\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.xticks(rotation=30, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_target_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    kpi_col: str,\n",
    "    target_col: str = \"hardware_target_canonical\",\n",
    "    agg: str = \"mean\",\n",
    "    title: Optional[str] = None,\n",
    "    top_n: Optional[int] = 20,\n",
    "):\n",
    "    data = df[[target_col, kpi_col]].dropna()\n",
    "    if data.empty:\n",
    "        print(f\"No data to compare for {kpi_col}.\")\n",
    "        return\n",
    "\n",
    "    grouped = data.groupby(target_col)[kpi_col].agg([agg, \"count\", \"std\"]).sort_values(agg, ascending=False)\n",
    "    if top_n:\n",
    "        grouped = grouped.head(top_n)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(grouped.index.astype(str), grouped[agg].values)\n",
    "    plt.xlabel(\"Target\")\n",
    "    plt.ylabel(f\"{agg}({kpi_col})\")\n",
    "    plt.title(title or f\"Target comparison: {kpi_col} ({agg})\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def plot_fidelity_vs_latency(\n",
    "    df: pd.DataFrame,\n",
    "    fidelity_col: str = \"kpi_fidelity\",\n",
    "    latency_col: str = \"kpi_latency_ms\",\n",
    "    target_col: str = \"hardware_target_canonical\",\n",
    "    title: str = \"Fidelity vs latency\",\n",
    "):\n",
    "    data = df[[target_col, fidelity_col, latency_col]].dropna()\n",
    "    if data.empty:\n",
    "        print(\"No data to plot fidelity vs latency.\")\n",
    "        return\n",
    "\n",
    "    plt.figure()\n",
    "    for tgt, g in data.groupby(target_col):\n",
    "        plt.scatter(g[latency_col], g[fidelity_col], label=str(tgt), alpha=0.8)\n",
    "    plt.xlabel(latency_col)\n",
    "    plt.ylabel(fidelity_col)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_pareto_frontier_latency_min_fidelity_max(\n",
    "    df: pd.DataFrame,\n",
    "    fidelity_col: str = \"kpi_fidelity\",\n",
    "    latency_col: str = \"kpi_latency_ms\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Pareto frontier where we want *lower latency* and *higher fidelity*.\n",
    "\n",
    "    Returns points on the frontier sorted by latency ascending.\n",
    "    \"\"\"\n",
    "    data = df[[latency_col, fidelity_col]].dropna().sort_values(latency_col)\n",
    "    if data.empty:\n",
    "        return data\n",
    "\n",
    "    best_fid = -np.inf\n",
    "    frontier_rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        fid = row[fidelity_col]\n",
    "        lat = row[latency_col]\n",
    "        if fid > best_fid:\n",
    "            frontier_rows.append((lat, fid))\n",
    "            best_fid = fid\n",
    "\n",
    "    frontier = pd.DataFrame(frontier_rows, columns=[latency_col, fidelity_col])\n",
    "    return frontier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b1304",
   "metadata": {},
   "source": [
    "## 1) KPI trends over time\n",
    "\n",
    "These plots show how KPIs evolve over time, optionally resampled to a regular cadence (daily by default).\n",
    "\n",
    "If you have many targets, consider narrowing to a subset first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a66ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to a subset of targets (optional)\n",
    "# example: keep only the 5 most common targets\n",
    "top_targets = df[\"hardware_target_canonical\"].value_counts().head(5).index.tolist()\n",
    "df_trend = df[df[\"hardware_target_canonical\"].isin(top_targets)].copy()\n",
    "\n",
    "# Plot fidelity trend (if available)\n",
    "if \"kpi_fidelity\" in df_trend.columns and df_trend[\"kpi_fidelity\"].notna().any():\n",
    "    plot_kpi_trend_over_time(\n",
    "        df_trend,\n",
    "        kpi_col=\"kpi_fidelity\",\n",
    "        resample_rule=TREND_RESAMPLE_RULE,\n",
    "        agg=TREND_AGG,\n",
    "        title=\"KPI trend: Fidelity\",\n",
    "    )\n",
    "else:\n",
    "    print(\"kpi_fidelity not available. Set FIDELITY_COL_OVERRIDE in the config cell and re-run.\")\n",
    "\n",
    "# Plot latency trend (if available)\n",
    "if \"kpi_latency_ms\" in df_trend.columns and df_trend[\"kpi_latency_ms\"].notna().any():\n",
    "    plot_kpi_trend_over_time(\n",
    "        df_trend,\n",
    "        kpi_col=\"kpi_latency_ms\",\n",
    "        resample_rule=TREND_RESAMPLE_RULE,\n",
    "        agg=TREND_AGG,\n",
    "        title=\"KPI trend: Latency (ms)\",\n",
    "    )\n",
    "else:\n",
    "    print(\"kpi_latency_ms not available. Set LATENCY_COL_OVERRIDE in the config cell and re-run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c636d7f",
   "metadata": {},
   "source": [
    "## 2) Target comparisons\n",
    "\n",
    "These charts compare KPI aggregates by target (mean by default).\n",
    "\n",
    "You’ll also get a small table back with `mean/count/std` to use in reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cde111",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_tables = {}\n",
    "\n",
    "if \"kpi_fidelity\" in df.columns and df[\"kpi_fidelity\"].notna().any():\n",
    "    comparison_tables[\"fidelity\"] = plot_target_comparison(\n",
    "        df, kpi_col=\"kpi_fidelity\", agg=\"mean\",\n",
    "        title=\"Target comparison: Mean fidelity\",\n",
    "        top_n=20,\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping fidelity comparison (kpi_fidelity missing).\")\n",
    "\n",
    "if \"kpi_latency_ms\" in df.columns and df[\"kpi_latency_ms\"].notna().any():\n",
    "    comparison_tables[\"latency_ms\"] = plot_target_comparison(\n",
    "        df, kpi_col=\"kpi_latency_ms\", agg=\"mean\",\n",
    "        title=\"Target comparison: Mean latency (ms)\",\n",
    "        top_n=20,\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping latency comparison (kpi_latency_ms missing).\")\n",
    "\n",
    "comparison_tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c257cc",
   "metadata": {},
   "source": [
    "## 3) Fidelity vs latency tradeoff\n",
    "\n",
    "Scatter plot for each run (optionally grouped by target).  \n",
    "At the bottom we also compute a **Pareto frontier** (lower latency, higher fidelity) across all runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\"kpi_fidelity\" in df.columns and df[\"kpi_fidelity\"].notna().any()\n",
    "    and \"kpi_latency_ms\" in df.columns and df[\"kpi_latency_ms\"].notna().any()):\n",
    "\n",
    "    plot_fidelity_vs_latency(df)\n",
    "\n",
    "    frontier = compute_pareto_frontier_latency_min_fidelity_max(df)\n",
    "    if not frontier.empty:\n",
    "        plt.figure()\n",
    "        plt.plot(frontier[\"kpi_latency_ms\"], frontier[\"kpi_fidelity\"], marker=\"o\")\n",
    "        plt.xlabel(\"kpi_latency_ms\")\n",
    "        plt.ylabel(\"kpi_fidelity\")\n",
    "        plt.title(\"Pareto frontier (lower latency, higher fidelity)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        display(frontier.head(20))\n",
    "else:\n",
    "    print(\"Need both kpi_fidelity and kpi_latency_ms to plot tradeoff. Override columns in config and re-run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25c3bc",
   "metadata": {},
   "source": [
    "## Export a cleaned dataset (optional)\n",
    "\n",
    "This can be useful to check into `exports/` or attach to an issue/PR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick exports directory relative to where this notebook lives\n",
    "exports_dir = Path(\"exports\") if Path(\"exports\").exists() else Path(\"../exports\")\n",
    "exports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clean_path = exports_dir / \"run_bundle_cleaned.csv\"\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(\"Wrote:\", clean_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5672ac",
   "metadata": {},
   "source": [
    "## Optional: generate a bundle from a running SynQc backend\n",
    "\n",
    "If you have the SynQc backend running locally (default `http://localhost:8001`), you can pull recent experiments\n",
    "and write them to `exports/run_bundle_from_api.json`.\n",
    "\n",
    "This is useful if you haven’t implemented a dedicated “export bundle” feature yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use:\n",
    "# API_BASE = \"http://localhost:8001\"\n",
    "# \n",
    "# try:\n",
    "#     import requests\n",
    "# except ImportError:\n",
    "#     # If you're in a clean environment:\n",
    "#     !pip -q install requests\n",
    "#     import requests\n",
    "# \n",
    "# recent = requests.get(f\"{API_BASE}/experiments/recent\", timeout=20).json()\n",
    "# # tolerate multiple shapes\n",
    "# runs = recent.get(\"experiments\") if isinstance(recent, dict) else recent\n",
    "# if runs is None and isinstance(recent, dict):\n",
    "#     runs = recent.get(\"runs\") or recent.get(\"items\") or recent.get(\"data\")\n",
    "# \n",
    "# bundle = {\n",
    "#     \"generated_at\": pd.Timestamp.utcnow().isoformat(),\n",
    "#     \"source\": f\"{API_BASE}/experiments/recent\",\n",
    "#     \"runs\": runs if isinstance(runs, list) else [runs],\n",
    "# }\n",
    "# \n",
    "# exports_dir = Path(\"exports\") if Path(\"exports\").exists() else Path(\"../exports\")\n",
    "# exports_dir.mkdir(parents=True, exist_ok=True)\n",
    "# out_path = exports_dir / \"run_bundle_from_api.json\"\n",
    "# out_path.write_text(json.dumps(bundle, indent=2), encoding=\"utf-8\")\n",
    "# print(\"Wrote:\", out_path.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
